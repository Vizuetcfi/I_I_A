---
Title: El pensamiento de la inteligencia artificial
Cretae by: Vizuet CF
Tiempo: 
Objetivos: Describir el concepto de la inteligencia artificial a través del paso del tiempo para entenderlo en la actualidad.
---

# I. Los inicios (*03:00 min*.)

Los inicios de la inteligencia artificial se remontan a la Edad Media y el Renacimiento, donde se desarrollaron autómatas  sofisticados que, sin embargo, tenían una funcionalidad limitada. En filosofo **Hobbes**, en su libro **Leviatán**, se pregunta si podríamos construir animales artificiales, y también si los autómatas podrían estar vivos. **Descartes** escribe que la vida es como los mecanismos de los relojes, sin embargo, considera que el alma no es mecánica, lo cual lleva a postular el dualismo, separando el cuerpo de la mente. **Leibniz**, por su parte, argumentó que el razonamiento podría reducirse a cálculos mecánicos. 

En la Edad Moderna, gracias a avances en relojería e ingeniería, se construyeron varios autómatas y androides, resaltando por su complejidad los de **Vaucanson** y de **Droz**. En **1818** se publicó "***Frankenstein o el moderno Prometeo***", de **Mary Shelley**, donde se popularizaron cuestiones relacionadas a la creación de criaturas similares a nosotros. 

Ya a mediados del siglo XX, con la construcción de las primeras computadoras electrónicas, se empezó a discutir la posibilidad de construir máquinas pensantes.
En cibernética, se estudió la comunicación y el control, tanto en animales como en máquinas. En 1943, el mexicano Arturo Rosenblueth,Norbert Wiener y Julian Bigelow publican un artículo donde discuten las propiedades del comportamiento con propósito, y concluyen que este podría ser producido por una máquina. Ese mismo año, Mcculloch y Pitts publican un artículo donde proponen un modelo matemático de redes neuronales, el cual sería la base para una de las áreas de inteligencia artificial. Durante esta época se construyeron algunos robots que exhibían ciertos comportamientos complejos. Sin embargo, aunque aún estamos lejos de la mayoría de los escenarios planteados por el cine y la televisión, quedan todavía muchos caminos por explorar.

---
# II. La prueba de Turing (*05:00 min*.)

Durante la Segunda Guerra Mundial, el británico Alan Turing pasó a la historia por haber definido funciones computables que descifraban las máquinas Enigma de los nazis; sin embargo, sus contribuciones a la humanidad no terminaron ahí. En 1950, Turing publicó en la revista Mind el artículo “Maquinaria computacional e inteligencia”; en este clásico, Turing se preguntaba si una máquina podría pensar. Sin embargo, como esta pregunta es ambigua, definió lo que se conoce como la Prueba de Turing, basada en el juego de la imitación.

## La prueba de Turing (*The imitation game*)

En el juego de la imitación, hay un hombre y una mujer que tratan de convencer a un interrogador. El hombre trata de hacerse pasar por la mujer, y la mujer trata de ayudar al interrogador a identificarla correctamente. En la prueba de Turing, una máquina trata de hacerse pasar por un humano, mientras que un humano ayuda al interrogador a identificar quién es quién. La idea es que, si consistentemente una máquina puede engañar a los interrogadores, entonces podremos decir que es inteligente. Esto no implica necesariamente que piensa como un humano, pero para fines prácticos, no podríamos distinguir si estamos conversando con un humano o con una máquina. 

Ya hay programas que han pasado la prueba de Turing desde los 90, durante una conversación simple introducían errores de mecanografía. Como las máquinas no se equivocan, interrogadores inexpertos son engañados fácilmente; pero estas máquinas pasaron la prueba de Turing equivocándose como humanos, no simulando nuestro pensamiento.

Más que engañar a los interrogadores, la cuestión es: ¿cómo podría un programa entablar una conversación en lenguaje natural? Para lograrlo, la computadora tendría que pasar por un proceso evolutivo, ya que Turing consideraba que sería imposible programar todo el conocimiento necesario para pasar su prueba. Sin embargo, se han usado los enfoques de aprendizaje automatizado y cómputo evolutivo desde entonces. Se podría argumentar que la prueba de Turing no mide tanto la  inteligencia en general, sino qué tanto una máquina puede imitar a los humanos. Pero ¿podríamos decir que hacer algo como humano te hace humano? 

## Funcionalismo y Esencialismo

Durante la historia, podemos identificar dos enfoques: funcionalista y esencialista. El ***funcionalista*** se enfoca en los procesos que determinan el futuro de un sistema, mientras que el ***esencialista*** se enfoca en sus componentes. 

En el enfoque **funcionalista***, podemos llamar a algo ***inteligente*** con que funcionalmente sea similar a lo que consideraríamos inteligente en un los humanos; no nos importa de qué manera lo haga, si emplea trucos o no. John Searle formuló la principal objeción **esencialista** con el “Problema del cuarto chino”.

### El problema del cuarto chino

Imaginemos a un inglés que está en un cuarto, donde recibe símbolos en chino que no entiende, y hay unos manuales que le dicen cómo manipular esos símbolos. Después de cierto tiempo, el inglés muestra la respuesta: más símbolos en chino, la cual es correcta. Pero el inglés no tiene idea de lo que significan los símbolos y, por lo tanto,
de lo que hizo. Algo similar sucede con las máquinas, pueden dar respuestas correctas pero no “*saben*” qué es lo que están haciendo.

Sin embargo, el problema del cuarto chino se puede aplicar también a los humanos: la mayor parte del tiempo actuamos sin estar conscientes de lo que estamos haciendo, y esto no implica que nuestro comportamiento no sea inteligente. Algunos programas ya han superado a los mejores humanos en algunos juegos, tales como ajedrez, go o póker. También hay pruebas que, probablemente, nos superen en los próximos años,
como carreras de autos; o en las próximas décadas, como en fútbol.

Sin embargo, hay otras tareas en las que no tenemos indicios de que puedan superarnos, tales como curarse, contar buenos chistes, cocinar creativamente, desenvolverse con éxito en tareas distintas a las definidas inicialmente por los humanos. En general, las máquinas son buenas sólo para resolver las tareas para las que fueron creadas. Los aviones no vuelan como aves ni como insectos, que, 
de hecho, son mucho más eficientes; pero si lo que nos interesa es volar, no importa tanto cómo lo logremos. De manera similar, si queremos construir sistemas inteligentes con cierto propósito, no importa si no funcionan de manera similar a los humanos
o a otros seres vivos. En otras palabras, ***la inteligencia artificial pragmáticamente es funcionalista***; de otra manera, nos perderemos en discusiones filosóficas.

---
# III. El nacimiento de una disciplina (*02:00 min*.)

Durante el verano de ***1956***, un grupo de investigadores se reunía en la Universidad Dartmouth, Estados Unidos, para por primera vez hablar sobre la inteligencia artificial. Aunque ya se había usado el término antes, fue durante este taller que nació como tal la disciplina. Durante los años siguientes, hubieron muchas innovaciones y se crearon grandes expectativas. Se dieron investigaciones importantes en MIT, Stanford y Edimburgo; se esperaba que en pocos años las máquinas pudieran hacer cosas que todavía no pueden hacer.

Las aplicaciones de inteligencia artificial en esta época se hicieron al razonamiento automatizado, prueba de teoremas, juegos, procesamiento de lenguaje natural y algunos robots. Según Marvin Minsky, inicialmente se enfocaron en búsquedas por ensayo y error y técnicas efectivas de aprendizaje. En los primeros años de la inteligencia artificial también se desarrolló el llamado ***conexionismo***, que continuando la abstracción de neuronas de McCulloch y Pitts de ***1943***, llevaron a lo que se conoció como ***redes neuronales***. El avance más exitoso de la época fue el ***perceptrón*** de Frank Rosenblatt, propuesto en  ***1958***, y el cual permitía aprendizaje por ensayo y error. 

Después de algunos avances, en ***1969***, Minsky y Papert, en su libro sobre perceptrones, notaron limitantes en la clasificación de estos sistemas de dos capas. Aunque se sabía y se propusieron perceptrones de más de dos capas que no tenían esta limitante, el fallecimiento temprano de Rosenblatt y la percepción de los limitantes de los perceptrones, inhibieron su investigación hasta mediados de ***1980***, donde se sentaron las bases de lo que hoy se conoce como ***aprendizaje automatizado***.

Desde ***1950***, se aplicaron a la búsqueda simulaciones de evolución, las cuales sentaron las bases para el desarrollo de los primeros algoritmos genéticos entre ***1960*** y ***1970***. Después de ***1962***, el enfoque cambió de aprendizaje al problema de representación del conocimiento; esto abrió la puerta a los sistemas basados en el conocimiento, y a los después llamados ***sistemas expertos***. Esto requirió de formalizaciones a través de distintos tipos de lógicas, y se crearon los lenguajes de programación específicos, tales como **Prolog** y **Lisp**, que todavía son usados. Fue durante estos primeros años de la disciplina, que se crearon grandes expectativas para el desarrollo y avance de la inteligencia artificial. Muchas de ellas, desafortunadamente, no pudieron ser cumplidas, pero aún quedaba mucho camino por recorrer.

--- 
# Fin de siglo (*04:00 min*).

Las ambiciosas expectativas no cumplidas de los primeros años de la inteligencia artificial, llevaron a lo que se conoció después como ***el invierno de la inteligencia artificial***, a mediados de ***1970***, que se caracterizó por reducción de fondos. Por ejemplo, se había empezado a trabajar en traducción de lenguajes usando tablas,
y después de algunos programas iniciales, se reconoció que estas técnicas no serían suficientes para lograr buenas traducciones automáticas, las cuales han mejorado bastante hasta nuestros días, pero todavía tienen sus limitantes. Límites más severos se detectaron en reconocimiento del lenguaje natural, en la comprensión del significado y también en conversaciones, por ejemplo, para pasar la prueba de Turing.

En ***1982***, Japón lanzó el proyecto de la **Quinta generación***, el cual pretendía revolucionar a las computadoras e invertir, durante diez años, sumas considerables de dinero. Aunque el proyecto nunca cumplió sus expectativas, revivió el interés en inteligencia artificial y sus promesas, financiando investigación y promoviendo la formación de recursos humanos en el área. Se desarrollaron sistemas expertos con aplicaciones exitosas en dominios particulares, tales como medicina. También se revivió el conexionismo, con trabajos como los de David Rumelhart, James McClelland y John Hopfield.

## Conexionismo y computacionalismo

Los avances en redes neuronales artificiales, motivaron críticas de gente que defendía a la ***inteligencia artificial simbólica***, tales como Jerry Fodor, Zenon Pylyshyn y Steven Pinker. Esto desató un debate entre los ***conexionistas***, que criticaban los límites de los sistemas basados en símbolos, preguntaban, por ejemplo: “¿dónde están los símbolos en un cerebro?”; y por otro lado, los ***computacionalistas***, ellos criticaban las redes neuronales como “cajas negras”, donde no se sabía dónde estaba su lógica. Aunque este debate todavía no ha concluido, en la práctica no es tan importante, ya que ambos enfoques han seguido aportando a la inteligencia artificial y no son incompatibles; simplemente son descripciones distintas de procesos cognitivos. Además, son computacionalmente equivalentes, esto quiere decir que se pueden construir sistemas de símbolos con redes neuronales y viceversa. Pero la inteligencia artificial simbólica, recibiría críticas adicionales además de las provenientes del conexionismo. 

Rodney Brooks, futuro director del Laboratorio de Inteligencia Artificial en MIT, publicó artículos a principios de  ***1990*** criticando la inteligencia artificial simbólica, ya que los robots que se habían construido hasta ese entonces eran muy limitados en su adaptación. ***Con inspiración en la etología***, la parte de la biología que estudia el comportamiento animal, y retomando conceptos de cibernética, ***se desarrolló una nueva inteligencia artificial con sistemas basados en el comportamiento, en lugar de basados en el conocimiento***; la cual también se complementó con el campo de la vida artificial, establecido a finales de ***1980***. En esta nueva inteligencia artificial,
cambió el enfoque del razonamiento a la interacción con el mundo a través de ciclos de percepción-acción. 

Veremos más detalles en el curso de Comportamiento adaptativo. En los 90, tanto en la inteligencia artificial **clásica**, basada en la manipulación de símbolos, como en los **nuevos** sistemas basados en el comportamiento, se propuso el concepto de “agente”;
‘inteligentes’ en la primera, ‘adaptativos y autónomos’ en la segunda. Un agente es una entidad que puede actuar en su entorno, los agentes inteligentes son guiados por reglas. Se definieron lógicas especiales para regular su interacción y han tenido aplicaciones en ingeniería de software, internet y otras áreas.

Los agentes autónomos adaptativos se han aplicado en robótica reactiva y vida artificial, entre otras.A fines de siglo, también se popularizó la investigación sobre emociones artificiales y cómputo afectivo. Marvin Minsky, en su influyente libro de 1986, "La sociedad de la mente", escribió: "La cuestión no es si las máquinas inteligentes pueden tener emociones, sino si las máquinas pueden ser inteligentes sin emociones".

En los años 90, Minsky empezó a compartir capítulos de lo que sería su libro "La máquina de las emociones", publicado en el 2006. Minsky decía que las emociones servían como reguladores del comportamiento. Dado que la inteligencia se caracteriza, entre otras cosas, por poder ajustarse a una situación, un sistema, en principio, debería poder tener distintas salidas para las mismas entradas. Las emociones ayudan a modular las respuestas, por ejemplo, tendemos a ser más tolerantes con las personas que más queremos. Tomando esto en consideración, a finales de los 90, Rosalind Picard, publicó un libro sobre cómputo afectivo, modelando emociones para regular interacciones humano-computador.

---
# Nuevo milenio (*02:00 min*).

Durante la última década se han dado avances notables en áreas de la inteligencia
artificial, como el reconocimiento de patrones; por ejemplo, detectar gatos en fotos, en autos autónomos y en juegos como Go. Sin embargo, todavía hay grandes limitantes, por ejemplo, en reconocimiento del lenguaje natural aún estamos muy lejos de poder tener conversaciones complejas con las máquinas. Siri, ¿por qué no me entiendes?
Buena pregunta. ¿Me lo podrías preguntar de otra forma?

Debido a los límites de la inteligencia artificial, se ha propuesto la “inteligencia artificial artificial”, donde se usa a humanos para resolver tareas que se les dificultan a las máquinas. Por ejemplo, las pruebas captcha para reconocer si un usuario no es un programa automatizado, de hecho una prueba inversa de Turing, ya que nosotros tenemos que convencer a una máquina de que somos humanos; explota nuestro reconocimiento de patrones para digitalizar libros y otros textos en imágenes.

A pesar de los éxitos de la inteligencia artificial, todos los sistemas son específicos. Es decir, un programa que juega a Go, no juega ajedrez; un programa que maneja un auto, no puede manejar una moto; un sistema que recomienda películas, no puede recomendar libros. Esta especificidad es lo que ha tratado de salvar la inteligencia artificial general: el poder construir sistemas que puedan aprender y adaptarse a una variedad de entornos novedosos. Sin embargo, la inteligencia artificial general, hasta el momento sólo ha producido especulaciones. 

---
# Referencias

**Course links**

> 1. [Los inicios](https://www.coursera.org/learn/sesenta-anos-inteligencia-artificial/lecture/yQ2Fa/los-inicios)
> 2. [La prueba de Turing](https://www.coursera.org/learn/sesenta-anos-inteligencia-artificial/lecture/eMJvC/la-prueba-de-turing)
> 3. [El nacimiento de una disciplina](https://www.coursera.org/learn/sesenta-anos-inteligencia-artificial/lecture/uSQeE/el-nacimiento-de-una-disciplina)

